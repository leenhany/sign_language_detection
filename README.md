# Hand Gesture Detection for Deaf Communication

## Features:
  - Real-time hand gesture recognition using webcam
  - Customizable gesture vocabulary via label files
  - Simple interface for data collection and model training
  - detect (A->Z) and some common words like (Hi,how are,nice,meet...)
  - Text output of recognized signs

## Demo
  ### Final real_time output
  
  ![Untitled video - Made with Clipchamp (2)](https://github.com/user-attachments/assets/80bc7164-0d44-40e3-84f4-b3c573915940)


## Installation:
  ### Prerequisites:
   <li/>Python 3.11
    <li/>pip install opencv-python
    <li/>pip install mediapipe
    <li/>pip install numpy
    <li/>pip install tensorflow

  ### Dependencies:
  
    ```bash
    pip install opencv-python mediapipe numpy tensorflow
    ```

## Project Structure:

```
â”œâ”€â”€ Collector/
â”‚   â”œâ”€â”€ collect.py          # Data collection script
â”‚   â”œâ”€â”€ data.csv           # Collected landmark data
â”‚   â””â”€â”€ label.csv           # Gesture labels
â”‚
â”œâ”€â”€ Reader/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ classifier.py   # Model inference class
â”‚   â”‚   â”œâ”€â”€ label.csv       # Gesture labels
â”‚   â”‚   â””â”€â”€ model.tflite    # Trained model
â”‚   â””â”€â”€ app.py              # Real-time detection script
â”‚
â””â”€â”€ Trainer/
    â”œâ”€â”€ model/
    â”‚   â”œâ”€â”€ data/          # Training data
    |   |    â”œâ”€â”€ data.csv
    |   |    â””â”€â”€  label.csv
    â”‚   â””â”€â”€ model.tflite    # Trained model
    â””â”€â”€ train_model.ipynb   # Model training notebook
```

## Usage:
### 1. Data Collection:
  ```bash
  python Collector/collect.py
  ```
  - Controls:
    - ENTER: Enter recording mode
    - Arrow keys: Select gesture label
    - SPACE: Start/stop auto-recording
    - BACKSPACE: Exit recording mode

### 2. Model Training:
  ```
  bash
  jupyter notebook Trainer/train_model.ipynb
  ```

### 3. Real-time Detection:
  ```
  bash
  python Reader/app.py
  ```
  - Controls:
    - 'c': Clear text
    - BACKSPACE: Delete last word
    - ESC: Exit

## Technical Details:
### Data Representation:
  - landmark_list: 21 (x,y) coordinates
  - Coordinates normalized relative to wrist
  - 42 input 

### Model Output:
  - N_hand_sign_id: Top N predicted indices
  - N_hand_sign_probs: Confidence scores
  - Minimum confidence: 10% (adjustable)
    
### Model conversion
  - convert model to tensorflow lite to be easy and fast in real_time
  - to give it to the Flutter team to connect it to the applications

## References:
- MediaPipe Hands: https://google.github.io/mediapipe/solutions/hands
- TensorFlow Lite for on-device inference

  
## ğŸ‘©â€ğŸ’» Author

**Leen Hany**  
GitHub: [@leenhany](https://github.com/leenhany)  
LinkedIn: [Leen Hany](https://www.linkedin.com/in/leen-hany-481850220/)

